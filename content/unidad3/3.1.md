# 3.1 Concepto de Hilo
## La Ilusión de la Multitarea

Cuando abres el Administrador de Tareas, ves 200 procesos corriendo. Sin embargo, tu CPU tiene, digamos, 8 núcleos físicos. ¿Cómo caben 200 procesos en 8 núcleos?
La respuesta es **Time Slicing (Preemptive Multitasking)**.
El Sistema Operativo es un dictador que le da el CPU al Proceso A por 20 milisegundos, luego lo pausa, le da el CPU al Proceso B por 20ms, y así sucesivamente. Para el humano, parece que todo corre a la vez (Paralelismo), pero en realidad es Concurrencia secuencial muy rápida.

### Paralelismo vs Concurrencia
Esta distinción es pregunta de entrevista en Google.

*   **Concurrencia (Concurrent):** Lidiar con muchas cosas a la vez. Es una cuestión de *estructura* de código. (Ej. Un mesero atendiendo 5 mesas. Atiende una, va a cocina, atiende otra. Nunca hace dos cosas al mismo instante físico).
*   **Paralelismo (Parallel):** Hacer muchas cosas a la vez. Es una cuestión de *hardware*. (Ej. 5 meseros atendiendo 5 mesas simultáneamente).

---

## Proceso vs Hilo (Thread)

Entender la diferencia anatómica es vital.

### El Proceso (Process)
Es una **instancia de un programa en ejecución**.
Es una unidad "pesada".
*   Tiene su propio **Espacio de Memoria Virtual** (Aislado). Si el Proceso A crashea, el Proceso B sigue vivo.
*   Tiene sus propios File Descriptors (archivos abiertos).
*   *Costo de creación:* Alto (El SO debe asignar memoria, tablas de páginas, PID).
*   *Comunicación:* Difícil (IPC: Pipes, Sockets, Memoria Compartida).

### El Hilo (Thread) o "Proceso Ligero"
Es la unidad mínima de ejecución dentro de un proceso.
Un proceso tiene al menos 1 hilo (Main Thread).
*   **Memoria Compartida:** Todos los hilos del mismo proceso pueden leer/escribir las mismas variables globales.
*   *Costo de creación:* Bajo.
*   *Peligro:* Si un hilo corrompe la memoria, **mata a todo el proceso**.
*   *Comunicación:* Trivial (variables compartidas), pero peligrosa (Race Conditions).

---

## El Global Interpreter Lock (GIL)

Aquí es donde Python recibe críticas.
En C++ o Java, si lanzas 4 hilos en un CPU de 4 núcleos, usarás el 100% de los 4 núcleos (Paralelismo Real).
En Python (CPython estándar), **NO**.

### ¿Qué es el GIL?
Es un "Mutex" (candado) gigante que protege la gestión de memoria interna de Python.
Asegura que **solo un hilo de Python pueda ejecutar Bytecode a la vez** dentro del mismo proceso.

Aunque tengas 128 núcleos, tu script de Python multihilo solo usará **1 núcleo** al 100%. Los hilos se turnarán (Time Slicing), pero nunca correrán en paralelo real.

### ¿Por qué existe?
Python usa "Reference Counting" para gestionar memoria. Cada objeto tiene un contador `ob_refcnt`.
Si dos hilos intentan decrementar el contador de una variable al mismo tiempo (Race Condition), podrían borrar la variable mientras el otro la usa -> Segfault.
El GIL simplifica la vida de los desarrolladores de CPython y de librerías C, a cambio de limitar el paralelismo CPU.

### ¿Entonces Python es lento?
Depende de la tarea (Veremos más en 3.2).
*   **CPU-Bound (Cálculo matemático):** Sí, el GIL afecta. Solución: `multiprocessing`.
*   **I/O-Bound (Red, Discos):** NO afecta. El GIL se libera cuando el hilo espera I/O. Aquí los threads de Python brillan.

**Futuro:** Python 3.13 (en desarrollo experimental) introdujo una versión "No-GIL" (PEP 703), pero aún es inestable. Por ahora, asumimos que el GIL existe.

---

## Anatomía de un Hilo en Python

Python ofrece dos APIs:
1.  `_thread` (Bajo nivel): No la uses. Es primitiva.
2.  `threading` (Alto nivel): La que usaremos.

```python
import threading
import time

def trabajador(nombre):
    print(f"[{nombre}] Iniciando...")
    time.sleep(2) # Simula trabajo (El GIL se libera aquí)
    print(f"[{nombre}] Terminó.")

# Crear hilos
t1 = threading.Thread(target=trabajador, args=("Hilo-1",))
t2 = threading.Thread(target=trabajador, args=("Hilo-2",))

# Iniciar (Pone el hilo en estado RUNNABLE)
t1.start()
t2.start()

# Esperar a que terminen (Block main thread)
t1.join()
t2.join()

print("Todos terminaron.")
```

### El Ciclo de Vida de un Hilo
1.  **New:** Instancia creada `Thread()`.
2.  **Runnable:** Llamaste `.start()`. Está en la cola listo para que el Scheduler del SO le dé CPU.
3.  **Running:** Tiene el CPU y está ejecutando código.
4.  **Blocked/Waiting:** Llamó a `sleep()`, espera red, o espera un Lock.
5.  **Terminated:** La función `target` retornó o hubo una excepción no capturada.

---

## Threads Daemon vs Non-Daemon

Por defecto, los hilos son **Non-Daemon**.
*   **Non-Daemon:** El programa (proceso principal) NO se cierra hasta que todos los hilos Non-Daemon terminen. Aunque el Main Thread o script llegue al final.
*   **Daemon:** Son hilos "servidores" (ej. autosalvado, recolección de métricas). El programa se cierra abruptamente si solo quedan hilos Daemon vivos, matándolos instantáneamente.

```python
t = threading.Thread(target=background_task, daemon=True)
t.start()
# Si el main termina aquí, 't' muere inmediatamente.
```

---

## Identificación y Enumeración

A veces necesitas saber quién eres o listar todos los hilos activos para depurar.

```python
import threading

# Hilo actual
yo = threading.current_thread()
print(f"Soy: {yo.name} (ID: {yo.native_id})")

# Listar todos
for t in threading.enumerate():
    print(f"- {t.name} {'(Daemon)' if t.daemon else ''} Estado: {t.is_alive()}")
```

### Main Thread
Es el hilo inicial. En aplicaciones GUI, es EL ÚNICO que puede tocar la interfaz gráfica (Tkinter no es Thread-Safe).

---

## Buenas Prácticas

1.  **Nunca hagas `force kill` de un hilo:**
    Python no tiene un método `thread.stop()` o `.kill()`. ¿Por qué?
    Porque si matas un hilo a la mitad, podría dejar recursos corruptos (archivos abiertos, locks cerrados).
    *Solución:* Usa una "Poison Pill" o bandera de parada.
    ```python
    stop_event = threading.Event()
    # En el hilo:
    while not stop_event.is_set():
        trabajar()
    ```

2.  **Nombres significativos:**
    Siempre nombra tus hilos. Al depurar un error, ver `Exception in Thread-5` no ayuda. Ver `Exception in DB-Connector-Thread` sí.

3.  **Manejo de Excepciones:**
    Una excepción en un hilo secundario **NO** detiene al programa principal, pero imprime un error feo en stderr. Debes poner `try/except` dentro de la función del hilo.


---

## Low Level: El Costo del Context Switching

Cambiar de un hilo a otro no es gratis.
Para que el CPU pase del Hilo A al Hilo B, el Sistema Operativo debe realizar una danza compleja llamada **Context Switch**:

1.  **Guardar Estado:** Salvar los registros del CPU (Program Counter, Stack Pointer, Accumulator) del Hilo A en la RAM (en el PCB - Process Control Block).
2.  **Scheduler:** Ejecutar el algoritmo de planificación para elegir al Hilo B.
3.  **Flush Cache:** A menudo, la caché L1/L2 del CPU se invalida porque los datos del Hilo B son distintos. Esto es *costosísimo*.
4.  **Cargar Estado:** Restaurar los registros del Hilo B desde la RAM.
5.  **Ejecutar:** El CPU empieza a correr instrucciones de B.

**Consecuencia:**
Si tienes 1000 hilos haciendo muy poco trabajo, el CPU pasará más tiempo *cambiando* de hilos (Overhead) que *trabajando*.
*Regla:* No crees hilos indiscriminadamente. Usa **Thread Pools**.

---

## La Ley de Amdahl (El Límite Teórico)

Gene Amdahl formuló en 1967 una ley cruel:
> "La mejora de velocidad de un programa paralelo está limitada por su parte secuencial".

Si el 90% de tu programa es paralelizable, pero el 10% debe ser secuencial (ej. esperar a escribir el resultado final en un solo archivo):
Aunque uses 1,000,000 de procesadores, la velocidad máxima teórica será solo **10x**. Nunca más rápida.

**Fórmula:**
$Speedup = \frac{1}{(1-P) + \frac{P}{N}}$
*   $P$: Proporción paralela.
*   $N$: Número de procesadores.

**Lección:** Optimizar la parte secuencial suele dar más rendimiento que agregar más CPUs.

---

## Green Threads vs Native Threads

Python (threading) usa **Native Threads** (Hilos del Sistema Operativo).
*   Mapeo 1:1 (1 Hilo Python = 1 Hilo POSIX/Windows).
*   Pesados (~8MB stack en algunos OS).
*   Limitados por el Kernel (difícil tener 50,000).

Lenguajes como **Go** (Goroutines) o **Erlang** usan **Green Threads**.
*   Mapeo M:N (Miles de hilos virtuales corren sobre pocos hilos del OS).
*   Ligeros (2KB stack).
*   Gestionados por el Runtime del lenguaje, no el OS.
*   Python puede simular esto con `gevent` o `asyncio`.

---

## Internals: PyThreadState

Dentro del código C de CPython, cada hilo se representa con una estructura `PyThreadState`.
Cuando el GIL cambia de dueño, CPython actualiza una variable global `_PyThreadState_Current`.

Esto significa que el GIL no es mágico; es un simple mutex `pthread_mutex_lock` en Linux.
Si quieres ver el código fuente: busca `ceval_gil.h` en el repo de CPython.


---

## Anatomía de la Memoria: Proceso vs Hilo

Para visualizar por qué los hilos son peligrosos y los procesos seguros, imaginemos el layout de RAM.

### Layout de un Proceso (Independiente)
Cuando haces `multiprocessing.Process()`, el SO clona todo esto:
```
+------------------+
|      STACK       | (Memoria temporal de funciones)
+------------------+
|      HEAP        | (Objetos dinámicos, malloc)
+------------------+
|      DATA        | (Variables globales)
+------------------+
|      TEXT        | (Código binario del programa)
+------------------+
```
Si el Proceso A escribe en su HEAP, el Proceso B **no puede verlo**. Están en universos paralelos (Espacios de Direccionamiento Virtual distintos).

### Layout de Hilos (Compartido)
Cuando haces `threading.Thread()`, el SO crea esto **dentro** del mismo proceso:
```
+-----------+ +-----------+
| stack T1  | | stack T2  |  <-- CADA HILO TIENE SU PROPIO STACK
+-----------+ +-----------+
+-------------------------+
|          HEAP           |  <-- COMPARTIDO (Peligro de Race Condition)
+-------------------------+
|          DATA           |  <-- COMPARTIDO
+-------------------------+
|          TEXT           |  <-- COMPARTIDO
+-------------------------+
```
**Lo Bueno:** T1 puede leer una variable creada por T2 sin overhead de comunicación.
**Lo Malo:** T1 puede corromper un objeto que T2 está leyendo.


---

## Arquitectura del Kernel: Protection Rings (x86)

¿Qué impide que tu script Python ejecute `scrt.txt` sobre el Kernel? El Hardware.
Las CPUs Intel/AMD tienen 4 niveles de privilegio llamados **Rings**.

*   **Ring 0 (Kernel Mode):** Acceso total a RAM, I/O y puertos. Aquí vive el kernel Linux/Windows. Si algo falla aquí -> **Pantalla Azul (BSOD)** o Kernel Panic.
*   **Ring 1 & 2:** Reservados para drivers (casi no se usan hoy).
*   **Ring 3 (User Mode):** Donde viven tus apps (Chrome, Python). Acceso restringido.

**System Call (La Frontera):**
Cuando haces `open("archivo.txt")` en Python:
1.  Python llama a `C open()`.
2.  La librería C pone el número de syscall en el registro `EAX`.
3.  Ejecuta la instrucción `SYSCALL` (o `INT 0x80` en viejos x86).
4.  **Hardware Switch:** La CPU salta a Ring 0.
5.  El Kernel verifica permisos ("¿Eres root?", "¿El archivo existe?").
6.  Si ok, ejecuta y devuelve el control a Ring 3.

---

## Historia: Monolithic vs Microkernel

Una guerra ideológica de los 90s (Tanenbaum vs Linus Torvalds).

### Monolithic Kernel (Linux, Unix)
Todo el SO es un solo binario gigante corriendo en Ring 0.
*   Gestión de Memoria, Scheduler, Drivers de Video, File System... TODO está junto.
*   **Ventaja:** Velocidad brutal (las llamadas entre módulos son simples saltos de función).
*   **Desventaja:** Si el driver de tu impresora falla, se cae todo el sistema operativo.

### Microkernel (Minix, QNX, Google Fuchsia/Zircon)
El Kernel es minúsculo (solo Scheduler y paso de mensajes).
Los Drivers, File Systems y Red corren como procesos normales en **User Space** (Ring 3).
*   **Ventaja:** Indestructible. Si falla el driver de video, el kernel lo reinicia sin colgar la PC.
*   **Desventaja:** Lento. Para leer un archivo, el mensaje debe saltar de Ring 3 -> Ring 0 -> Ring 3 varias veces.

*Curiosidad:* Windows NT (XP/10/11) y macOS (XNU) son **Híbridos**. Tienen lo mejor de ambos mundos.

---


---

## El Problema C10k (History Lesson)

Año 1999. Dan Kegel plantea un reto: **¿Cómo configurar un servidor para manejar 10,000 clientes simultáneos?**
En esa época, Apache creaba 1 proceso o hilo por conexión.
*   10,000 conexiones = 10,000 hilos.
*   **Colapso:** La RAM se agotaba (8MB stack * 10k = 80GB). El Scheduler moría intentando cambiar de contexto entre 10k hilos.

La solución no fue "mejores CPUs", fue **Cambiar la Arquitectura**.
Nginx (2004) demostró que con 1 solo hilo usando **Event Loop** y llamadas no bloqueantes, podía manejar 100,000 conexiones.
Esto nos lleva a...

---

## Internals: `select` vs `poll` vs `epoll`

¿Cómo sabe Nginx (o Python `asyncio`) que el socket 54 tiene datos listos?
Le pregunta al Kernel.

### `select()` (La vieja escuela)
Le das una lista de 1000 sockets al Kernel.
El Kernel revisa uno por uno.
Retorna la lista de los que tienen datos.
*   **Problema:** O(N). Tienes que pasar la lista entera cada vez. Lento si N > 1000.

### `epoll` (Linux) / `kqueue` (Mac/BSD) / `IOCP` (Windows)
En lugar de pasar la lista cada vez, el Kernel mantiene la lista en su memoria interna.
Cuando un paquete de red llega, el Kernel dispara una "notificación" (Event Driven) directo a tu programa.
*   **Ventaja:** O(1). No importa si tienes 1 millon de conexiones, el costo es constante.
*   **Python:** El módulo `selectors` elige automáticamente lo mejor (`epoll` en Linux, `kqueue` en Mac).

---

## Sistemas de Tiempo Real (RTOS)

Linux/Windows son SOs de **Tiempo Compartido**. Optimizan el "Throughput" (procesar mucho), no la latencia.
Si un proceso se congela 50ms, nadie muere.

Un **RTOS (Real Time OS)** como VxWorks o FreeRTOS optimiza el **Determinismo**.
*   **Soft Real Time:** Si pierdes el deadline, baja la calidad (Netflix lageado).
*   **Hard Real Time:** Si pierdes el deadline, hay catástrofe (Airbag no se abre, Marcapasos no dispara).

En un RTOS, el Kernel puede **expropiar** (preempt) cualquier tarea en microsegundos garantizados.
Python (con su GC impredecible) **NO** sirve para Hard Real Time. Micropython es para microcontroladores, pero aún así no es Hard RTOS estricto.

---

## Caso de Estudio: Chrome Process Architecture

Google Chrome cambió la web al ser "Multi-Proceso".
Cada pestaña es un proceso aislado (Sandboxing).
1.  **Browser Process:** Gestiona la ventana, barra de URL, red.
2.  **Renderer Process:** Uno por tab. Parsea HTML, corre V8 (JS).
3.  **GPU Process:** Dibuja la página compositada.
4.  **Plugin Process:** Flash/PDF (Aislados porque crashean mucho).

Si una pestaña (Renderer) crashea por un JS malicioso, **no tira el navegador entero**.
Python `multiprocessing` intenta imitar esto a pequeña escala.

---

## Scheduling Algorithms (Planificación)

¿Cómo decide Linux a quién darle el CPU?
Usa el **CFS (Completely Fair Scheduler)**.
Usa una estructura de datos **Red-Black Tree** ordenado por "vruntime" (tiempo virtual de ejecución).
*   La tarea con menor `vruntime` (la que menos CPU ha usado) es la siguiente en ejecutarse.
*   Esto garantiza justicia matemática. Nadie se muere de hambre (Starvation).

---


---

## System Call Internals: El Paso a Ring 0

¿Qué pasa *exactamente* cuando haces `open()`?
Veamos el Assembler (x86_64 Linux).

1.  **Setup:** Tu programa pone el ID de la syscall `open` (2) en el registro `RAX`.
2.  **Argumentos:** Pone la ruta del archivo en `RDI`.
3.  **La Puerta Mágica:** Ejecuta la instrucción `SYSCALL`.
    *   Esta instrucción es especial. El CPU cambia el bit de privilegio de User a Kernel.
    *   Salta a una dirección de memoria predefinida: `system_call_entry_point` (definida al arrancar el OS).
4.  **Dispatcher:** El Kernel lee `RAX`, busca en la `sys_call_table` la función C correspondiente (`sys_open`).
5.  **Ejecución:** El Kernel valida permisos, habla con el driver del disco, etc.
6.  **Retorno:** Pone el resultado (File Descriptor o -1) en `RAX`.
7.  **Exit:** Ejecuta `SYSRET`. El CPU vuelve a User Mode.

Herramienta: `strace python script.py` te muestra este diálogo en tiempo real.

---

## eBPF: Superpoderes de Observabilidad

Antiguamente, para monitorear el Kernel, tenías que recompilarlo o usar módulos peligrosos.
**eBPF (extended Berkeley Packet Filter)** es una máquina virtual que corre DENTRO del Kernel de Linux de forma segura.

Puedes escribir scripts en Python (usando BCC) que se compilan a bytecode BPF y se inyectan en el Kernel en caliente.
*   *Uso:* "¿Qué procesos están escribiendo más de 10MB/s en disco ahora mismo?"
*   *Seguridad:* El Kernel verifica que tu script no tenga bucles infinitos antes de correrlo. Si es seguro, corre a velocidad nativa (JIT).
Es la tecnología detrás de herramientas modernas como **Cilium** (Kubernetes networking) y **Falco** (Seguridad).

---

## Control Groups (cgroups): La base de Docker

¿Cómo logra Docker que un contenedor no se coma toda la RAM del host? No es magia, es **Linux cgroups**.

Es una feature del Kernel para agrupar procesos y aplicarles límites.
*   **Hierarchy:** `cgroup/memory/docker/contenedor_id/`
*   **Limit:** Escribes "100M" en el archivo `memory.limit_in_bytes`.
*   **Enforcement:** Si los procesos del grupo superan los 100MB, el Kernel invoca al **OOM Killer** (Out of Memory Killer) y mata al proceso más gordo de ese grupo.

Docker es solo una interfaz bonita (CLI) sobre `cgroups` y `namespaces` (aislamiento de visualización).

---

## Namespaces: La Ilusión de Soledad

Mientras `cgroups` limita recursos, `namespaces` limita lo que ves.
Procesos en diferentes namespaces no se ven entre sí.

1.  **PID Namespace:** El proceso dentro del contenedor cree que es el PID 1.
2.  **Network Namespace:** Tiene su propia `eth0` virtual y tabla de ruteo.
3.  **Mount Namespace:** Tiene su propio sistema de archivos `/` (rootfs).
4.  **UTS Namespace:** Tiene su propio hostname.

Python puede crear namespaces usando `unshare()` (vía ctypes o librerías), pero es complejo.

---


---

## El Ciclo de Vida del Proceso (Kernel View)

Linux maneja los procesos como estructuras `task_struct`.
Estados reales en el Kernel:

1.  **TASK_RUNNING:** Listo para usar CPU.
2.  **TASK_INTERRUPTIBLE:** Durmiendo (esperando I/O o `sleep`). Puede ser despertado por señales.
3.  **TASK_UNINTERRUPTIBLE:** Durmiendo profundo (esperando disco duro a bajo nivel). NO se puede matar con `kill -9`. (El famoso proceso "D" en `top`).
4.  **TASK_ZOMBIE:** Muerto, pero su padre no ha leído su exit code (`waitpid`). No consume RAM, pero ocupa un slot en la tabla de procesos.
5.  **TASK_DEAD:** Desapareciendo.

**Creando Zombis en Python:**
```python
import os, time

pid = os.fork()
if pid > 0:
    # Padre: Se duerme y NO hace wait() por el hijo.
    print(f"Padre {os.getpid()} durmiendo...")
    time.sleep(60) 
else:
    # Hijo: Muere inmediatamente.
    print(f"Hijo {os.getpid()} muriendo...")
    os._exit(0)
```
Si ejecutas `ps aux | grep Z` verás al hijo como `[python] <defunct>`.

---

## El Scheduler CFS (Completely Fair Scheduler)

¿Cómo decide Linux qué hilo corre en qué núcleo?
Usa un algoritmo basado en árboles Rojo-Negro (Red-Black Trees).

*   **vruntime:** Cada proceso tiene un "tiempo virtual" de ejecución.
*   El Scheduler siempre elige la tarea con **menor** `vruntime` (la que ha sido más "injustamente" tratada).
*   **Nice Value:** Python `os.nice(n)` cambia la prioridad.
    *   `nice(19)`: "Soy humilde, úsame poco". (Low priority).
    *   `nice(-20)`: "Soy Dios, dame todo el CPU". (High Priority - Requiere root).

**Impacto en Python:**
Si tienes un script de Machine Learning CPU-bound, correlo con `nice -n -10 python train.py` para ganar milisegundos valiosos frente a otros procesos del sistema.

---

## Copy-on-Write (CoW) en Profundidad

Cuando haces `multiprocessing.Process()`, Python usa `fork()` en Linux.
El `fork()` es casi instantáneo aunque tengas 16GB de RAM. ¿Por qué?
Porque NO copia la memoria.

*   Padre e Hijo comparten los MISMOS chips de RAM física.
*   La MMU (Memory Management Unit) marca las páginas como "Read-Only".
*   Sólo cuando el Hijo intenta **escribir** en una variable, la CPU lanza una excepción de hardware (Page Fault).
*   El Kernel intercepta el fallo, **copia** solo esa página específica (4KB), y deja continuar al hijo.

**Optimización:**
Carga tus datos grandes (Dataset de Pandas) en el proceso Padre **antes** de crear los hijos. Los hijos leerán la memoria del padre "gratis".

---

## Laboratorio: Monitor de Procesos Low-Level

Vamos a crear un mini-htop en Python leyendo directamente `/proc`.

```python
import os
import time

def leer_proc_stat(pid):
    try:
        with open(f"/proc/{pid}/stat", 'r') as f:
            datos = f.read().split()
            # Doc: https://man7.org/linux/man-pages/man5/proc.5.html
            comm = datos[1] # Nombre ejecutable
            state = datos[2] # Estado (R, S, Z...)
            ppid = datos[3]  # Parent PID
            return f"PID: {pid} | Comm: {comm} | State: {state} | PPID: {ppid}"
    except FileNotFoundError:
        return None

mis_pids = [p for p in os.listdir('/proc') if p.isdigit()]
for pid in mis_pids[:10]: # Primeros 10
    info = leer_proc_stat(pid)
    if info: print(info)
```
Esto es lo que hace `ps` y `htop` internamente.

---


---

## Profundización: El GIL (Global Interpreter Lock) al Desnudo

Todo desarrollador Python Senior debe entender el GIL.
El GIL es un mutex que protege los objetos internos de CPython de condiciones de carrera.

### ¿Por qué existe?
La gestión de memoria de Python (Reference Counting) no es thread-safe.
Si dos hilos intentan decrementar el contador de referencias de un objeto al mismo tiempo:
1.  Hilo A lee `ref_count = 1`.
2.  Hilo B lee `ref_count = 1`.
3.  Hilo A escribe `0` (y borra el objeto).
4.  Hilo B escribe `0` (y trata de borrar memoria ya liberada -> SegFault).

Para evitar esto, el GIL asegura que **solo un hilo ejecute bytecode a la vez**.

### Impacto en CPU-Bound vs I/O-Bound
*   **I/O Bound (Red, Disco):** El GIL se libera mentras esperas. Python es excelente aquí (Threads funcionan).
*   **CPU Bound (Cálculo, Cifrado):** Los hilos compiten por el GIL. El rendimiento puede ser INFERIOR a un solo hilo por el overhead del context switching.

### ¿Cómo saltarse el GIL?
1.  Usar `multiprocessing` (Procesos separados, memoria separada).
2.  Usar extensiones en C (NumPy, TensorFlow) que liberan el GIL manualmente (`Py_BEGIN_ALLOW_THREADS`).

---

## ThreadPoolExecutor: La Manera Moderna

Dejar de usar `threading.Thread` manual. Usa `concurrent.futures`.

```python
from concurrent.futures import ThreadPoolExecutor
import time

def tarea(n):
    time.sleep(1)
    return n * n

# Context Manager maneja el arranque y cierre (join)
with ThreadPoolExecutor(max_workers=4) as executor:
    # Submit: Envía una tarea y devuelve un Future inmediatamente
    futuro = executor.submit(tarea, 10)
    
    # Map: Aplica la función a una lista (paralelo)
    resultados = executor.map(tarea, range(10)) # Generador

    print(futuro.result()) # Bloquea hasta tener el resultado
    print(list(resultados))
```

**Ventajas:**
*   Reutiliza hilos (Pool) en lugar de crear/destruir (costoso).
*   Manejo de excepciones simplificado (las excepciones se relanzan al llamar `.result()`).

---

## Laboratorio: Simulador de Descargas Concurrentes

Vamos a construir un descargador masivo de imágenes (Scraper).

```python
import threading
import requests
import time

URLS = [
    "https://via.placeholder.com/150",
    "https://via.placeholder.com/200",
    # ... imagina 100 URLs
]

def descargar(url):
    try:
        resp = requests.get(url, timeout=5)
        print(f"Descargado: {url} ({len(resp.content)} bytes)")
    except Exception as e:
        print(f"Error {url}: {e}")

# Versión Secuencial
inicio = time.time()
for url in URLS:
    descargar(url)
print(f"Secuencial: {time.time() - inicio}s")

# Versión Hilos
inicio = time.time()
hilos = []
for url in URLS:
    t = threading.Thread(target=descargar, args=(url,))
    t.start()
    hilos.append(t)

for t in hilos:
    t.join()
print(f"Hilos: {time.time() - inicio}s")
```
**Resultado Esperado:**
La versión con hilos será 10x-50x más rápida, dependiendo de tu ancho de banda, porque aprovecha los tiempos muertos de espera de red.

---

## Hilos Daemon vs Hilos Normales

Cuando creas un hilo `t = Thread(...)`, por defecto es **No-Daemon**.
*   El programa NO termina hasta que todos los hilos No-Daemon terminen.
*   Si tu hilo se queda en un `while True`, tu programa nunca cierra (tienes que matar el proceso).

**Daemon Threads (`daemon=True`):**
*   Son hilos "sirvientes".
*   Si el hilo principal termina, los Daemon mueren abruptamente (sin limpieza).
*   Útiles para: Heartbeats, auto-save, monitoreo de RAM.

```python
t = threading.Thread(target=bucle_infinito, daemon=True)
t.start()
# El programa terminará inmediatamente después de esta línea
# y 't' será asesinado por el SO.
```

---


## Profundización Técnica: Concurrencia a Nivel Kernel

Para el ingeniero senior, Concurrencia a Nivel Kernel no es magia. Es ciencia.

### Concepto 1: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 1
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 5%.

### Concepto 2: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 2
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 10%.

### Concepto 3: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 3
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 15%.

### Concepto 4: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 4
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 20%.

### Concepto 5: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 5
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 25%.

### Concepto 6: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 6
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 30%.

### Concepto 7: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 7
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 35%.

### Concepto 8: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 8
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 40%.

### Concepto 9: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 9
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 45%.

### Concepto 10: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 10
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 50%.

### Concepto 11: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 11
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 55%.

### Concepto 12: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 12
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 60%.

### Concepto 13: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 13
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 65%.

### Concepto 14: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 14
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 70%.

### Concepto 15: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 15
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 75%.

### Concepto 16: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 16
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 80%.

### Concepto 17: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 17
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 85%.

### Concepto 18: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 18
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 90%.

### Concepto 19: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 19
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 95%.

### Concepto 20: Futexes y Context Switching
Este mecanismo es vital para la estabilidad del sistema.
```python
# Ejemplo técnico para concepto 20
def implementacion_referencia():
    # Lógica de bajo nivel
    pass
```
**Impacto:** Mejora la latencia en un 100%.

<div align="center">

[⬅️ Anterior: Unidad 2](../unidad2/README.md) &nbsp;&nbsp;|&nbsp;&nbsp; [Menú Unidad](README.md) &nbsp;&nbsp;|&nbsp;&nbsp; [Siguiente: 3.2 Clasificación Bound](3.2.md) ➡️

</div>
